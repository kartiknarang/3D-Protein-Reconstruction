# 3D-Protein-Reconstruction
Reconstruct 3D representation of proteins using Neural Radiance Fields

The dataset that we’re using is from AlphaFold which has predictions for the protein structures across a list of 48 organisms. We started with Methanocaldococcus jannaschii since the dataset is small, but we will test our methods with at least 3 of such predicted structures. These structures are stored in PDB (Protein Data bank) files, which contain atomic coordinates and connectivity between these atoms among other useful information. Using these predictions, we devise the following baseline method:

Given a folder of Methanocaldococcus jannaschii PDBs, we do the following for each PDB. We first generate 3 random angles or views of the protein and store these coordinates. We vertically stack these three views together and train a machine learning model to learn the mapping between the input views and the original protein structure. We then use the trained model to generate the reconstructed coordinates for the protein structure and create a visualization using point clouds using Plotly to see how closely it resembles the original protein.

The second method is using NeRFs. In this case, we will be learning a mapping from 2D input images to a 5D radiance fields. We’ll represent the protein structure as a set of 3D points and associated color information. We then prepare the training data where the input views are represented as 2D projections of the 3D structure along with their color information. The NeRF model will take in 5D input (X, Y, Z, Viewing Direction, Viewing Elevation) and output RGB color and density values. After training, in order to test this reconstruction method, we’ll give it a 2D query point (pixel) in an output image, and we’ll use the NeRD model to predict the 5D radiance field information for that point and reconstruct the 3D position of the protein structure.
